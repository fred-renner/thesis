
\chapter{Discussion and Outlook}
The analysis conducted in this thesis demonstrates for the first time the potential and advantages of the \ac{neos} approach over traditional methods in an actual full particle physics analysis search for Higgs boson pairs in a boosted topology. The results highlight the significant role of uncertainties when optimizing neural networks which outputs are used for the evaluation of statistical tests in a particle physics experiment. It has been shown that \ac{neos} provides a robust framework for systematic-aware neural network training and optimization. Even though this analysis refrains from unblinding as at the time of writing not all uncertainties were calibrated, the result is without loss of generality and it is not simulation alone as collision data are used in the background estimate.

\section{Discussion}
The analysis shows a considerable improvement in expected cross-section limits for the \textsc{neos} approach compared to traditional methods. Specifically, for the $\kappa_{2V} = 0$ signal hypothesis, the \textsc{neos} approach demonstrates approximately a two-fold improvement in reducing the limits compared to both a maximum likelihood fit to the invariant mass of the Higgs pair system \mhh and a \ac{bce}-trained neural network. A similar enhancement is observed for the \textsc{neos} method when evaluated on the \ac{sm} signal. This significant improvement underscores \textsc{neos}' ability in optimizing analysis performance when accounting for systematic uncertainties. The current best observed (expected) constraints at a 95\% confidence level on the \ktwov coupling are determined by the \ac{cms} collaboration \citep{PhysRevLett.131.041803} as $ 0.62 < \kappa_{2V} < 1.41 (0.66 < \kappa_{2V}< 1.37)$ for the same Higgs pair decay topology used in this analysis. Latest results from when using the \ac{atlas} detector with the same topology are  $0.52 < \kappa_{2v}< 1.52 (0.32 < \kappa_{2v}< 1.71)$ at 95\% confidence level \citep{atlascollaboration2024searchpairproductionboosted}. This thesis improves the expected constraints using the latest advancements in object reconstruction, namely \ac{ufo} jets and the GN2X tagger $b-$ tagger, and using \ac{neos} to $(0.55 < \kappa_{2v}< 1.48)$ at the 95\% confidence level.  Even though \ac{neos} represents an approximation it comes without any caveats on the final results as implemented into the traditional analysis chain for the determination of cross-sectional limits it outperforms classical methods.

This highlights the crux of the situation as optimizing on Asimov significance alone cannot account for uncertainties and has thus decreased performance compared to \ac{neos}. If \ac{neos} is not applicable it can be advisable when searching for a new process to implement a loss function that optimizes the Asimov significance which does not require any \ac{neos} methods given the observed correlation of the Asimov significance with the \ac{neos} performance. A study on this \citep{elwood2018direct} has also shown that such an approach can give improved results for analyses dominated by systematic uncertainties \citep{elwood2018direct}.

One of the most prominent features exploited in this work with \ac{neos} is the ability to draw a conclusion on the existence of a physical process by inputting only the basic reconstructed objects and variables associated to an event or process of interesting and setting up analysis optimization parameters either believed encapsulating the process of interest like cuts or paramaters used to estimate the background and letting neos optimize them such that it can almost be referred to as `autoanalysis'. As all of these parameters are correlated in non-trivial ways it not only reduces/removes (a word in between the two here) unwanted researcher bias introduced via successive manual optimization which strategically does not lead necessarily to a global optimum and might even introduce bias by optimizing on potential artifacts. It also opens the opportunity for any particle physics analysis to find a quick estimate of the sensitivity of a given process given \ac{mc} simulations or observed data.

In particular in searches that involve hadronic final states depend crucially on a background estimate for the plethora of potentially accompanied \ac{qcd} processes in the selection. It has been shown that if a strategy for a data-driven background estimate and method of estimating the uncertainty on this background estimate is given \ac{neos} can effectively reduce this uncertainty which simplifies the analysis effort a lot as for example a large effort went into the background estimation of the resolved version of the $HH\rightarrow 4b$ analysis. Notably this uncertainty is overestimated with the same extraction methods for the \mhh and \ac{bce}-trained model whereas \ac{neos} optimizes this uncertainty to the maximum as the pull shows almost perfect expected uncertainty behavior.



\section{Outlook}
Given the promising results obtained with \textsc{neos}, several avenues for future research and development are proposed. A priori as this is a reiteration of the same analysis there is an opportunity for improvement when setting up the analysis strategy as reconstruction methods and analysis objects were changed to the latest available versions. This is evident when observing the \ac{sr} in the $m_\text{H1}-m_\text{H2}$ massplane and also the definition of the \ac{vr} and \ac{cr} which could ideally follow a concentrically increased \ac{sr} shape. This could be taken as far as giving \ac{neos} the choice of the size of these regions.

This highlights another huge feature of \ac{neos} that everything which is parameterizable can become part of the optimization and one can add arbitrary operations as long as they are differentiable. This can be any parameter or several \acp{nn} serving dedicated tasks within the analysis chain. Of course there is some natural limit as an infinitely complex function will make the gradient descent hard. The gradient landscape can become quickly complex when only considering that e.g. the cut optimization adds a sigmoid differentiation per sample and uncertainty per cut.

The uncertainty estimate on the background estimate in this work is as studies have shown one of the most important ones the analyzer with the largest influence on the extracted limits the analyzer is controlling. It is executed most conservatively and could benefit from a definition with improved statistics.

Early studies tried the bin edges as parameters to be optimized and tested different numbers of bins but hardly showed any differences on the limits. As this might be interesting for conservative analyses for \ac{neos} there seems to be a degeneracy as it is potentially capable of placing events into desired bins such that there is no need to optimize the bin edges.

Moreover \ac{neos} is far from being optimized. It could be experimented with different optimizers or trying a different architecture like a transformer. In addition hyperparameters such as the learning rate or influential parameters like the slopes on the sigmoid function used in estimating a cut or the bandwidth controlling the histogram shape resulting from the kernel density estimates could be studied further. Furthermore there are no regularization techniques like dropout, weight decay or batch normalization applied which help in finding lower minimum. Also second-order methods could be tried like Newton's method or L-BFGS that use second-order derivatives which help in finding lower minima.

Even a comparably small impact of $\sim\qty[]{3}{\percent}$ on the final limits a future analysis could consider generating \ac{mc} samples for several \ktwov couplings in order to avoid signal hypothesis mismodelling due to the linear combination of samples. Further one could easily reoptimize neos for each of these hypotheses.

The uncertainty estimation for the background estimates relies on very little statistics on the double Higgs tagged \ac{vr}. A true validation of the uncertainty estimate would require: letting \ac{neos} optimize in an additional orthogonal region to the \ac{vr} and then extract the uncertainty from the actual \ac{vr}. An idea here would be to split the \ac{vr} into four quadrants. It can be argued that in this way an overfitting of the background uncertainty to the actual \ac{vr} as been done by the optimization could be avoided. \red{However in principle this is no different to a calibration.}

Another interesting path would be to study relations in the input data and the ones found by the \ac{neos} model. From this analyzers might learn which event properties are of matter and how are they associated to each other. In addition to studying associations found by the \ac{nn} a dimensional reduction techniques like t-distributed stochastic neighbor embedding (t-SNE) could be used to study associations of the input data. Another exploration of interest would be to study what kind of events fall into which bins.

Another interesting path after \ac{neos} or maybe the follow-up approach by the same proposers as \ac{neos} or next generation \ac{neos} could be \textit{Learning Optimal Test Statistics in the Presence of Nuisance Parameters} by \citet{heinrich2022learningoptimalteststatistics}. This techniques bypasses the profile likelihood building comes up with a test statistic are shown to be equivalent to those obtained from profile likelihood ratios, which are traditionally used but computationally expensive.




\red{
    binned transfer factor might be worth exploring --> is running.
}




\section{Conclusion}
In the context of $b$-tagging small-$R$ jets the potential offered by muons from semileptonic decays were explored with an up to 40\% improved background rejection. The \ac{neos} approach represents a significant advancement in the optimization of neural networks for particle physics experiments, particularly in the presence of complex systematic uncertainties. In particular `autoanalysis' makes manual optimization efforts obsolete such that a researcher can concentrate on the analysis strategy. The results obtained in this thesis highlight its potential to enhance the sensitivity and precision of analyses, opening new avenues for future research and discovery in particle physics.



% Prospects of non-resonant Higgs pair production
% at the HL-LHC and HE-LHC
% https://iopscience.iop.org/article/10.1088/1742-6596/1690/1/012149/pdf


% https://indico.cern.ch/event/1359386/contributions/5723345/attachments/2786832/4859053/ATLASdihiggs2024jan.pdf 

